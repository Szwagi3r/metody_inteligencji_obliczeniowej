{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodano uczenie metodą propagacji błędu wraz z podejściem batchowym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, weights, bias, num_outputs=1, num_hidden=1, activation_function=\"sigmoid\"):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation_function = activation_function\n",
    "        self.num_hidden = len(weights) - 1\n",
    "        self.num_neurons = [weights[0].shape[0]]\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            self.num_neurons.append(weights[i].shape[1])\n",
    "        self.num_inputs = self.num_neurons[0]\n",
    "        self.num_outputs = self.num_neurons[-1]\n",
    "\n",
    "        # verify if dimensions are correct\n",
    "        for i in range(len(weights) - 1):\n",
    "            if (weights[i].shape[1] != weights[i + 1].shape[0]):\n",
    "                print(\"Weights' dimensions between layers \" + str(i) + \" and \" + str(i + 1) + \"are incorrect!\")\n",
    "        for i in range(len(bias)):\n",
    "            if (weights[i].shape[1] != bias[i].size):\n",
    "                print(\"Bias size in layer \" + str(i) + \" is incorrect!\")\n",
    "\n",
    "        # derivatives of weights and biases used in backpropagation\n",
    "        self.dw = []\n",
    "        self.db = []\n",
    "\n",
    "    def forward_prop(self, inputs):\n",
    "        \"\"\"\n",
    "        performs forward propagation\n",
    "        \"\"\"\n",
    "        # activations and linear combinations passed to activation function\n",
    "        self.a = []\n",
    "        self.z = []\n",
    "\n",
    "        activation_function = self.getActivationFunction(self.activation_function)\n",
    "        activations = inputs\n",
    "        self.a.append(activations)\n",
    "        for i in range(self.num_hidden):\n",
    "            outputs = np.dot(activations, self.weights[i]) + self.bias[i]\n",
    "            self.z.append(outputs)\n",
    "            activations = activation_function(outputs)\n",
    "            self.a.append(activations)\n",
    "\n",
    "        results = np.dot(activations, self.weights[self.num_hidden]) + self.bias[self.num_hidden]\n",
    "        self.z.append(results)\n",
    "        self.a.append(results)\n",
    "        return results\n",
    "\n",
    "    def backpropagation(self, y):\n",
    "        deltas = [None] * len(self.weights)\n",
    "        deltas[-1] = y - self.a[-1]\n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            deltas[i] = (self.weights[i + 1].dot(deltas[i + 1].T) * (\n",
    "                self.getDerivitiveActivationFunction(self.activation_function)(self.z[i]).T)).T\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        db = [d.T.dot(np.ones((batch_size, 1))).T / float(batch_size) for d in deltas]\n",
    "        dw = [(d.T.dot(self.a[i])).T / float(batch_size) for i, d in enumerate(deltas)]\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, x, y, batch_size = 20, epochs = 500, learning_rate = 0.1, verbose = False):\n",
    "        for epoch in range(epochs):\n",
    "            i = 0\n",
    "            while i < len(y):\n",
    "                x_batch = x[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                i = i + batch_size\n",
    "                self.forward_prop(x_batch)\n",
    "                dw, db = self.backpropagation(y_batch)\n",
    "                self.weights = [w + learning_rate * dweight for w, dweight in zip(self.weights, dw)]\n",
    "                self.bias = [w + learning_rate * dbias for w, dbias in zip(self.bias, db)]\n",
    "                if verbose or epoch in [0,epochs-1]:\n",
    "                    print(\"Error in epoch {} = {}\".format(epoch, np.linalg.norm(self.a[-1] - y_batch)))\n",
    "\n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if (name == 'sigmoid'):\n",
    "            return lambda x: np.exp(x) / (1 + np.exp(x))\n",
    "        elif (name == 'linear'):\n",
    "            return lambda x: x\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "\n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if (name == 'sigmoid'):\n",
    "            sig = lambda x: np.exp(x) / (1 + np.exp(x))\n",
    "            return lambda x: sig(x) * (1 - sig(x))\n",
    "        elif (name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1\n",
    "\n",
    "    def show_attributes(self):\n",
    "        \"\"\"\n",
    "    gives basic information about the neural network\n",
    "    \"\"\"\n",
    "        print(\"Neural Network attributes:\")\n",
    "        print(\"-------------------------\")\n",
    "        print(\"Number of neurons in layers: {}\".format(self.num_neurons))\n",
    "        print(\"Number of predictors: {}\".format(self.num_inputs))\n",
    "        print(\"Number of hidden layers: {}\".format(self.num_hidden))\n",
    "        print(\"Number of targets: {}\".format(self.num_outputs))\n",
    "        print(\"Activations function used: {}\".format(self.activation_function))\n",
    "        return\n",
    "\n",
    "\n",
    "def generate_random_mlp(num_inputs, num_hidden, num_targets, start=-10, stop=10):\n",
    "    \"\"\"\n",
    "    generates mlp with random weights given number of neurons in each layer\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    bias = []\n",
    "    weights.append(np.random.uniform(start, stop, num_inputs * num_hidden[0]).reshape(num_inputs, -1))\n",
    "    for i in range(len(num_hidden) - 1):\n",
    "        weights.append(np.random.uniform(start, stop, num_hidden[i] * num_hidden[i + 1]).reshape(num_hidden[i], -1))\n",
    "        bias.append(np.random.uniform(start, stop, num_hidden[i]).reshape(1, -1))\n",
    "    weights.append(np.random.uniform(start, stop, num_hidden[-1] * num_targets).reshape(-1, num_targets))\n",
    "    bias.append(np.random.uniform(start, stop, num_hidden[-1]).reshape(1, -1))\n",
    "    bias.append(np.random.uniform(start, stop, num_targets).reshape(1, -1))\n",
    "    mlp = MLP(weights, bias)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zainicjujmy losową sieć i sprawdźmy jak nauczy się wzoru paraboli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymlp = generate_random_mlp(1,[5,10,5],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network attributes:\n",
      "-------------------------\n",
      "Number of neurons in layers: [1, 5, 10, 5, 1]\n",
      "Number of predictors: 1\n",
      "Number of hidden layers: 3\n",
      "Number of targets: 1\n",
      "Activations function used: sigmoid\n"
     ]
    }
   ],
   "source": [
    "mymlp.show_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('~/Documents/Sem6/MIO/datasets/regression/square-simple-training.csv', index_col=0)\n",
    "test_df = pd.read_csv('~/Documents/Sem6/MIO/datasets/regression/square-simple-test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.171543</td>\n",
       "      <td>-127.351580</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025201</td>\n",
       "      <td>-129.942844</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.368991</td>\n",
       "      <td>38.672367</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.907390</td>\n",
       "      <td>197.432191</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011129</td>\n",
       "      <td>-129.988852</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.315377</td>\n",
       "      <td>25.719403</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1.196968</td>\n",
       "      <td>-1.054107</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.544766</td>\n",
       "      <td>84.767303</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.441051</td>\n",
       "      <td>-112.492699</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.025176</td>\n",
       "      <td>-129.942957</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x           y  split\n",
       "1   -0.171543 -127.351580  train\n",
       "2    0.025201 -129.942844  train\n",
       "3   -1.368991   38.672367  train\n",
       "4    1.907390  197.432191  train\n",
       "5    0.011129 -129.988852  train\n",
       "..        ...         ...    ...\n",
       "96   1.315377   25.719403   test\n",
       "97  -1.196968   -1.054107   test\n",
       "98   1.544766   84.767303   test\n",
       "99   0.441051 -112.492699   test\n",
       "100 -0.025176 -129.942957   test\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['split'] = 'train'\n",
    "test_df['split'] = 'test'\n",
    "df = pd.concat([train_df, test_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train split')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.scatterplot(x='x', y='y', data=train_df)\n",
    "plt.title('Train split', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymlp = generate_random_mlp(1, [10,20], 1, start=-100, stop=100)\n",
    "mymlp.show_attributes()\n",
    "mymlp.train(np.asarray(train_df['x']).reshape(-1,1), np.asarray(train_df['y']).reshape(-1,1), 200, 10000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mymlp.forward_prop(np.asarray(test_df['x']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='x', y='y', data=test_df)\n",
    "sns.scatterplot(x=test_df['x'], y=np.concatenate(result))\n",
    "plt.title('Test performance', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieć uczy się prawidłowo, jednak to dobrego ustawienia wag potrzebny jest mały krok oraz duża liczba iteracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdżmy w jakim stopniu propagacja wsteczna błędu przyda się do ulepszenia sieci z poprzedniego tygodnia. Wyglądała wtedy następująco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = [np.array([[1, 1, 1, 1, 1]]), np.array([[900], [200], [0], [-200], [-900]])]\n",
    "bias_1 = [np.array([-2, -1, 0, 1, 2]), np.array([650])]\n",
    "mlp_1 = MLP(weights=weights_1, bias=bias_1)\n",
    "mlp_1.show_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mlp_1.forward_prop(np.asarray(train_df['x']).reshape(-1,1))\n",
    "sns.scatterplot(x='x', y='y', data=train_df)\n",
    "sns.scatterplot(x=train_df['x'], y=np.concatenate(res))\n",
    "plt.title('Train performance', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po użyciu zaimplementowanej techniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1.train(np.asarray(train_df['x']).reshape(-1,1), np.asarray(train_df['y']).reshape(-1,1), 200, 1000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mlp_1.forward_prop(np.asarray(test_df['x']).reshape(-1,1))\n",
    "sns.scatterplot(x='x', y='y', data=test_df)\n",
    "sns.scatterplot(x=test_df['x'], y=np.concatenate(res))\n",
    "plt.title('Test performance', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wypadł zdecydowanie dobrze. Przy 1000 epokach sieć lepiej przewiduje obserwacje na \"ogonach\" paraboli."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
